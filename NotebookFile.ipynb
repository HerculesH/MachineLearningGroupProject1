{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/HercHja/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/HercHja/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "class Splitter(object):\n",
    "    def __init__(self):\n",
    "        self.nltk_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self, text):\n",
    "        \"\"\"\n",
    "        input format: a paragraph of text\n",
    "        output format: a list of lists of words.\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        \"\"\"\n",
    "        sentences = self.nltk_splitter.tokenize(text)\n",
    "        tokenized_sentences = [self.nltk_tokenizer.tokenize(sent) for sent in sentences]\n",
    "        return tokenized_sentences\n",
    "\n",
    "\n",
    "class POSTagger(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def pos_tag(self, sentences):\n",
    "        \"\"\"\n",
    "        input format: list of lists of words\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        output format: list of lists of tagged tokens. Each tagged tokens has a\n",
    "        form, a lemma, and a list of tags\n",
    "            e.g: [[('this', 'this', ['DT']), ('is', 'be', ['VB']), ('a', 'a', ['DT']), ('sentence', 'sentence', ['NN'])],\n",
    "                    [('this', 'this', ['DT']), ('is', 'be', ['VB']), ('another', 'another', ['DT']), ('one', 'one', ['CARD'])]]\n",
    "        \"\"\"\n",
    "\n",
    "        pos = [nltk.pos_tag(sentence) for sentence in sentences]\n",
    "        #adapt format\n",
    "        pos = [[(word, word, [postag]) for (word, postag) in sentence] for sentence in pos]\n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "class DictionaryTagger(object):\n",
    "    def __init__(self, dictionary_paths):\n",
    "        files = [open(path, 'r') for path in dictionary_paths]\n",
    "        dictionaries = [yaml.load(dict_file) for dict_file in files]\n",
    "        map(lambda x: x.close(), files)\n",
    "        self.dictionary = {}\n",
    "        self.max_key_size = 0\n",
    "        for curr_dict in dictionaries:\n",
    "            for key in curr_dict:\n",
    "                if key in self.dictionary:\n",
    "                    self.dictionary[key].extend(curr_dict[key])\n",
    "                else:\n",
    "                    self.dictionary[key] = curr_dict[key]\n",
    "                    self.max_key_size = max(self.max_key_size, len(key))\n",
    "\n",
    "    def tag(self, postagged_sentences):\n",
    "        return [self.tag_sentence(sentence) for sentence in postagged_sentences]\n",
    "\n",
    "    def tag_sentence(self, sentence, tag_with_lemmas=False):\n",
    "        \"\"\"\n",
    "        the result is only one tagging of all the possible ones.\n",
    "        The resulting tagging is determined by these two priority rules:\n",
    "            - longest matches have higher priority\n",
    "            - search is made from left to right\n",
    "        \"\"\"\n",
    "        tag_sentence = []\n",
    "        N = len(sentence)\n",
    "        if self.max_key_size == 0:\n",
    "            self.max_key_size = N\n",
    "        i = 0\n",
    "        while (i < N):\n",
    "            j = min(i + self.max_key_size, N) #avoid overflow\n",
    "            tagged = False\n",
    "            while (j > i):\n",
    "                expression_form = ' '.join([word[0] for word in sentence[i:j]]).lower()\n",
    "                expression_lemma = ' '.join([word[1] for word in sentence[i:j]]).lower()\n",
    "                if tag_with_lemmas:\n",
    "                    literal = expression_lemma\n",
    "                else:\n",
    "                    literal = expression_form\n",
    "                if literal in self.dictionary:\n",
    "                    #self.logger.debug(\"found: %s\" % literal)\n",
    "                    is_single_token = j - i == 1\n",
    "                    original_position = i\n",
    "                    i = j\n",
    "                    taggings = [tag for tag in self.dictionary[literal]]\n",
    "                    tagged_expression = (expression_form, expression_lemma, taggings)\n",
    "                    if is_single_token: #if the tagged literal is a single token, conserve its previous taggings:\n",
    "                        original_token_tagging = sentence[original_position][2]\n",
    "                        tagged_expression[2].extend(original_token_tagging)\n",
    "                    tag_sentence.append(tagged_expression)\n",
    "                    tagged = True\n",
    "                else:\n",
    "                    j = j - 1\n",
    "            if not tagged:\n",
    "                tag_sentence.append(sentence[i])\n",
    "                i += 1\n",
    "        return tag_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicttagger = DictionaryTagger(['positive.yml', 'negative.yml'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_of(sentiment):\n",
    "    if sentiment == 'positive': return 1\n",
    "    if sentiment == 'negative': return -1\n",
    "    return 0\n",
    "\n",
    "def getSentenceSentiment(sentence):\n",
    "    text = sentence\n",
    "    splitter = Splitter()\n",
    "    postagger = POSTagger()\n",
    "    splitted_sentences = splitter.split(text)\n",
    "    pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n",
    "    dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "\n",
    "    return print(sum ([value_of(tag) for sentence in dict_tagged_sentences for token in sentence for tag in token[2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0  1467811592  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY  \\\n",
      "0       0  1467812771  Mon Apr 06 22:20:19 PDT 2009  NO_QUERY   \n",
      "1       0  1467813782  Mon Apr 06 22:20:34 PDT 2009  NO_QUERY   \n",
      "2       0  1467814783  Mon Apr 06 22:20:50 PDT 2009  NO_QUERY   \n",
      "3       0  1467815988  Mon Apr 06 22:21:09 PDT 2009  NO_QUERY   \n",
      "4       0  1467817502  Mon Apr 06 22:21:32 PDT 2009  NO_QUERY   \n",
      "5       0  1467819650  Mon Apr 06 22:22:05 PDT 2009  NO_QUERY   \n",
      "6       0  1467821085  Mon Apr 06 22:22:26 PDT 2009  NO_QUERY   \n",
      "7       0  1467822522  Mon Apr 06 22:22:49 PDT 2009  NO_QUERY   \n",
      "8       0  1467824199  Mon Apr 06 22:23:15 PDT 2009  NO_QUERY   \n",
      "9       0  1467825642  Mon Apr 06 22:23:39 PDT 2009  NO_QUERY   \n",
      "10      0  1467833736  Mon Apr 06 22:25:45 PDT 2009  NO_QUERY   \n",
      "11      0  1467834265  Mon Apr 06 22:25:54 PDT 2009  NO_QUERY   \n",
      "12      0  1467835305  Mon Apr 06 22:26:10 PDT 2009  NO_QUERY   \n",
      "13      0  1467836500  Mon Apr 06 22:26:28 PDT 2009  NO_QUERY   \n",
      "14      0  1467837470  Mon Apr 06 22:26:43 PDT 2009  NO_QUERY   \n",
      "15      0  1467838188  Mon Apr 06 22:26:54 PDT 2009  NO_QUERY   \n",
      "16      0  1467839007  Mon Apr 06 22:27:08 PDT 2009  NO_QUERY   \n",
      "17      0  1467840016  Mon Apr 06 22:27:25 PDT 2009  NO_QUERY   \n",
      "18      0  1467841897  Mon Apr 06 22:27:56 PDT 2009  NO_QUERY   \n",
      "19      0  1467842568  Mon Apr 06 22:28:07 PDT 2009  NO_QUERY   \n",
      "20      0  1467843470  Mon Apr 06 22:28:21 PDT 2009  NO_QUERY   \n",
      "21      0  1467844097  Mon Apr 06 22:28:33 PDT 2009  NO_QUERY   \n",
      "22      0  1467852031  Mon Apr 06 22:30:34 PDT 2009  NO_QUERY   \n",
      "23      0  1467853479  Mon Apr 06 22:30:56 PDT 2009  NO_QUERY   \n",
      "24      0  1467855812  Mon Apr 06 22:31:31 PDT 2009  NO_QUERY   \n",
      "25      0  1467856632  Mon Apr 06 22:31:45 PDT 2009  NO_QUERY   \n",
      "26      0  1467857511  Mon Apr 06 22:31:59 PDT 2009  NO_QUERY   \n",
      "27      0  1467859025  Mon Apr 06 22:32:22 PDT 2009  NO_QUERY   \n",
      "28      0  1467859820  Mon Apr 06 22:32:36 PDT 2009  NO_QUERY   \n",
      "29      0  1467861522  Mon Apr 06 22:33:00 PDT 2009  NO_QUERY   \n",
      "...    ..         ...                           ...       ...   \n",
      "251166  4  2193551085  Tue Jun 16 08:36:40 PDT 2009  NO_QUERY   \n",
      "251167  4  2193551398  Tue Jun 16 08:36:41 PDT 2009  NO_QUERY   \n",
      "251168  4  2193551571  Tue Jun 16 08:36:42 PDT 2009  NO_QUERY   \n",
      "251169  4  2193551788  Tue Jun 16 08:36:44 PDT 2009  NO_QUERY   \n",
      "251170  4  2193552033  Tue Jun 16 08:36:44 PDT 2009  NO_QUERY   \n",
      "251171  4  2193552370  Tue Jun 16 08:36:46 PDT 2009  NO_QUERY   \n",
      "251172  4  2193552501  Tue Jun 16 08:36:47 PDT 2009  NO_QUERY   \n",
      "251173  4  2193552958  Tue Jun 16 08:36:49 PDT 2009  NO_QUERY   \n",
      "251174  4  2193553267  Tue Jun 16 08:36:50 PDT 2009  NO_QUERY   \n",
      "251175  4  2193553464  Tue Jun 16 08:36:51 PDT 2009  NO_QUERY   \n",
      "251176  4  2193553605  Tue Jun 16 08:36:52 PDT 2009  NO_QUERY   \n",
      "251177  4  2193554053  Tue Jun 16 08:36:54 PDT 2009  NO_QUERY   \n",
      "251178  4  2193554329  Tue Jun 16 08:36:56 PDT 2009  NO_QUERY   \n",
      "251179  4  2193554652  Tue Jun 16 08:36:57 PDT 2009  NO_QUERY   \n",
      "251180  4  2193575000  Tue Jun 16 08:38:38 PDT 2009  NO_QUERY   \n",
      "251181  4  2193575236  Tue Jun 16 08:38:39 PDT 2009  NO_QUERY   \n",
      "251182  4  2193575729  Tue Jun 16 08:38:41 PDT 2009  NO_QUERY   \n",
      "251183  4  2193575955  Tue Jun 16 08:38:42 PDT 2009  NO_QUERY   \n",
      "251184  4  2193576360  Tue Jun 16 08:38:44 PDT 2009  NO_QUERY   \n",
      "251185  4  2193576534  Tue Jun 16 08:38:45 PDT 2009  NO_QUERY   \n",
      "251186  4  2193576815  Tue Jun 16 08:38:47 PDT 2009  NO_QUERY   \n",
      "251187  4  2193577232  Tue Jun 16 08:38:46 PDT 2009  NO_QUERY   \n",
      "251188  4  2193577592  Tue Jun 16 08:38:51 PDT 2009  NO_QUERY   \n",
      "251189  4  2193577680  Tue Jun 16 08:38:51 PDT 2009  NO_QUERY   \n",
      "251190  4  2193577941  Tue Jun 16 08:38:53 PDT 2009  NO_QUERY   \n",
      "251191  4  2193578269  Tue Jun 16 08:38:54 PDT 2009  NO_QUERY   \n",
      "251192  4  2193578395  Tue Jun 16 08:38:55 PDT 2009  NO_QUERY   \n",
      "251193  4  2193578847  Tue Jun 16 08:38:57 PDT 2009  NO_QUERY   \n",
      "251194  4  2193579249  Tue Jun 16 08:38:59 PDT 2009  NO_QUERY   \n",
      "251195  4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
      "\n",
      "                mybirch                                        Need a hug   \n",
      "0       robrobbierobert  @octolinz16 It it counts, idk why I did either...  \n",
      "1             gi_gi_bee  @FakerPattyPattz Oh dear. Were you drinking ou...  \n",
      "2           KatieAngell  Just going to cry myself to sleep after watchi...  \n",
      "3              merisssa  thought sleeping in was an option tomorrow but...  \n",
      "4               Tmttq86  @fleurylis I don't either. Its depressing. I d...  \n",
      "5             antzpantz  @Viennah Yay! I'm happy for you with your job!...  \n",
      "6        crzy_cdn_bulas  our duck and chicken are taking wayyy too long...  \n",
      "7                Jenn_L  Where did u move to?  I thought u were already...  \n",
      "8             adri_mane  @Starrbby too bad I won't be around I lost my ...  \n",
      "9              timmelko  @ninjen I'm sure you're right...    I need to ...  \n",
      "10         MagicalMason                           No new csi tonight.  FML  \n",
      "11      mike_webster_au                        @markhardy1974 Me too  #itm  \n",
      "12         MissLaura317  @januarycrimson Sorry, babe!!  My fam annoys m...  \n",
      "13       natalieantipas  so rylee,grace...wana go steve's party or not?...  \n",
      "14           annette414                        watching &quot;House&quot;   \n",
      "15          jess_higley  @marykatherine_q i know! I heard it this after...  \n",
      "16               eyezup  @mercedesashley Damn! The grind is inspiration...  \n",
      "17           BustaBusta  I know my life has been flipped upside down wh...  \n",
      "18           aaronrothe  @chelserlynn haha its so cooooold in the d! an...  \n",
      "19      josiahmcdermott                               i miss kenny powers   \n",
      "20      KandiConnection    @mrsaintnick hey! i'm leavin in the morning...   \n",
      "21             sew_cute  Going to sleep. Hoping tomorrow is a better day.   \n",
      "22                kscud  getting sick  time for some hot tea, studying,...  \n",
      "23      ringleaderkanon                           I feel bad for doing it   \n",
      "24           easyausguy                       @marieclr I was serious  LOL  \n",
      "25            BrettFair        i don't see the big deal with this website   \n",
      "26            fatkat309  @dannyvegasbaby danny im upset that i wasnt he...  \n",
      "27           LeeseEllen  So many channels.... yet so so boring... lazy ...  \n",
      "28        msbutt3rfly14                        spencer is not a good guy.   \n",
      "29         bovinemammal  Job Interview in Cardiff today, wish me luck! ...  \n",
      "...                 ...                                                ...  \n",
      "251166         saraeden       @lilkup it's a relief. Conflict exhausts me   \n",
      "251167        siierra86                     new tatoo  love you megggiiiee  \n",
      "251168           crside  @dfinchalicious  It ends up like that when goo...  \n",
      "251169    annamadeleine  @alexandervelky that's polite version - i only...  \n",
      "251170    josephranseth  Good morning everyone! I hope you take some ti...  \n",
      "251171       omgitsafox              @Common_Pigeon You're quite welcome.   \n",
      "251172      Jay_Cousins  Nice out, but If I am going to leave here at 6...  \n",
      "251173    angelxwarrior  @kazzc22 no probs kazz hun,i think theres a ne...  \n",
      "251174      kevinmyriad  I'm gonna go take a shower and turn in the las...  \n",
      "251175     UrBaN_eLySsE  @FoSho174 LMAO @ capture the flag..bless their...  \n",
      "251176     josie_barnes               @Bruno108 I would like some please!   \n",
      "251177          RaniJoy  busy day at work!  So happy the Burkharts are ...  \n",
      "251178     chloooebartz  going to south streeeet with kate, hopefully m...  \n",
      "251179         JenAvila                @unworthysaint Are you having fun?   \n",
      "251180       Ten_Tenths  Post your Le Mans 2009 pics: .. I didn't reall...  \n",
      "251181            drjan            my cluster is back and I'm a happy man   \n",
      "251182    herreramarisa      @drummeroy nice new name  how you holding up?  \n",
      "251183   misscaroline16                       scrubbed into surgery today   \n",
      "251184     MandyStocker  dropping molly off getting ice cream with Aaro...  \n",
      "251185        Essence05  @ohsosweettreats Omg, I loveeeee frosted flake...  \n",
      "251186      sarasmile13                 @gone2dmb agreed!  Soulful eyes.    \n",
      "251187           nrgins  @CharlotteSpeaks So, wasn't that funny re. the...  \n",
      "251188   ChelseaLately_  any ideaZ on what to get dad for father's day ...  \n",
      "251189   WParenthetical  @ashinynewcoin yeah, that'd be the one  sorry ...  \n",
      "251190          lexi234  got home an hour ago ate lunch watched some tv...  \n",
      "251191       millerslab          @MikeJonesPhoto Congrats Mike  Way to go!  \n",
      "251192      LaurenMoo10                          @shebbs123 i second that   \n",
      "251193       RobFoxKerr  You heard it here first -- We're having a girl...  \n",
      "251194    razzberry5594                              WOOOOO! Xbox is back   \n",
      "251195      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
      "\n",
      "[251196 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('NLP-classification-training-data.csv', encoding = \"ISO-8859-1\")\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
